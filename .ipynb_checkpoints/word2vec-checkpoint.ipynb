{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /anaconda3/lib/python3.7/site-packages (4.31.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting gensim\n",
      "  Using cached https://files.pythonhosted.org/packages/70/cf/87b25b265d23498b2b70ce873495cf7ef91394c4baff240210e26f3bc18a/gensim-3.8.3-cp37-cp37m-macosx_10_9_x86_64.whl\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /anaconda3/lib/python3.7/site-packages (from gensim) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /anaconda3/lib/python3.7/site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /anaconda3/lib/python3.7/site-packages (from gensim) (1.16.2)\n",
      "Collecting boto3 (from smart-open>=1.8.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/3c/f4/41c1d8a69b07b2a087a7e552cbed21111ff36706fec2f1ba9983fba95771/boto3-1.14.20-py2.py3-none-any.whl\n",
      "Requirement already satisfied: boto in /anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.49.0)\n",
      "Requirement already satisfied: requests in /anaconda3/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.21.0)\n",
      "Collecting botocore<1.18.0,>=1.17.20 (from boto3->smart-open>=1.8.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/87/a6/1710181d97a6763ccced7f69fff8beea751633af2a101c3d02826cf4acce/botocore-1.17.20-py2.py3-none-any.whl\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.8.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
      "Collecting s3transfer<0.4.0,>=0.3.0 (from boto3->smart-open>=1.8.1->gensim)\n",
      "  Using cached https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.3.9)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /anaconda3/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.20->boto3->smart-open>=1.8.1->gensim) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /anaconda3/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.20->boto3->smart-open>=1.8.1->gensim) (0.14)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "Successfully installed boto3-1.14.20 botocore-1.17.20 gensim-3.8.3 jmespath-0.10.0 s3transfer-0.3.3 smart-open-2.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm\n",
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from arena_util import remove_seen\n",
    "from arena_util import write_json\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import WordEmbeddingsKeyedVectors\n",
    "from gensim.parsing.preprocessing import preprocess_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('train.json', encoding='UTF-8')\n",
    "val = pd.read_json('val.json', encoding='UTF-8')\n",
    "song_meta = pd.read_json('./song_meta.json', encoding='UTF-8')\n",
    "most_results = pd.read_json('results.json', encoding='UTF-8')\n",
    "min_count = 10\n",
    "size = 100\n",
    "windows = 100\n",
    "sg = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2v_model = WordEmbeddingsKeyedVectors(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas apply 진행상황 보여주기 위한 모듈\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- song_name_dic  \n",
    "    plylst id와 song name을 분해한 list를 매칭(plylst id - \\[song1 분해, song2 분해, ... \\])\n",
    "- total(pandas.Series)  \n",
    "    song name 분해한 것 + tag + plylst 제목 분해한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115071/115071 [02:35<00:00, 737.77it/s] \n"
     ]
    }
   ],
   "source": [
    "song_dic = {} # plylst id - song id\n",
    "song_name_dic = {} # plylst id - [preprocessed song name]\n",
    "tag_dic = {} # plylst id - tag\n",
    "data = pd.concat([train, val])\n",
    "data = data.set_index('id')\n",
    "\n",
    "song_dic = data['songs'].to_dict()\n",
    "tag_dic = data['tags'].to_dict()\n",
    "data['song_name_token'] = data['songs'].progress_apply(lambda songs : sum([preprocess_string(song_meta.loc[song_id, 'song_name']) for song_id in songs], []))\n",
    "song_name_dic = data['song_name_token'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115071/115071 [00:07<00:00, 15857.17it/s]\n"
     ]
    }
   ],
   "source": [
    "data = data.reset_index()\n",
    "total = data.progress_apply(lambda x : song_name_dic[x['id']] + preprocess_string(x['plylst_title']) + tag_dic[x['id']], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(total, min_count = min_count, size = size, window = windows, sg = sg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "115071it [02:31, 759.14it/s] \n"
     ]
    }
   ],
   "source": [
    "ID = []   \n",
    "vec = []\n",
    "embedd = {}\n",
    "for index, q in tqdm(pd.concat([train, val]).iterrows()):\n",
    "    tmp_vec = 0\n",
    "    for song_word in song_name_dic[q['id']]:\n",
    "        try:\n",
    "            tmp_vec += w2v_model.wv.get_vector(song_word) / len(song_name_dic[q['id']])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    for tag in tag_dic[q['id']]:\n",
    "        try:\n",
    "            tmp_vec += w2v_model.wv.get_vector(tag) / len(tag_dic[q['id']])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    for title_word in preprocess_string(q['plylst_title']):\n",
    "        try:\n",
    "            tmp_vec += 2 * w2v_model.wv.get_vector(title_word) / len(preprocess_string(q['plylst_title']))\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    if type(tmp_vec) != int:\n",
    "        embedd[str(q['id'])] = tmp_vec\n",
    "        ID.append(str(q['id']))  \n",
    "        vec.append(tmp_vec)\n",
    "\n",
    "p2v_model.add(ID, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "707989it [03:36, 3269.72it/s]\n"
     ]
    }
   ],
   "source": [
    "s2v_model = WordEmbeddingsKeyedVectors(size)\n",
    "\n",
    "ID_song = []\n",
    "vec_song = []\n",
    "song_embed = {}\n",
    "for index, q in tqdm(song_meta.iterrows()):\n",
    "    tmp_vec = 0\n",
    "    for word in preprocess_string(q['song_name']):\n",
    "        try:\n",
    "            tmp_vec += w2v_model.wv.get_vector(word) / len(preprocess_string( q['song_name']))\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if type(tmp_vec) != int:\n",
    "        song_embed[q['song_name']] = tmp_vec\n",
    "        ID_song.append(str(q['id']))\n",
    "        vec_song.append(tmp_vec)\n",
    "        \n",
    "s2v_model.add(ID_song, vec_song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vivaldi : Concerto No.4 In F Minor `L`Inverno` RV297 `Winter` - Largo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('60452', 0.9957778453826904),\n",
       " ('381790', 0.9950672388076782),\n",
       " ('252871', 0.9922040700912476),\n",
       " ('372944', 0.9878601431846619),\n",
       " ('351307', 0.9867910742759705),\n",
       " ('248161', 0.9867508411407471),\n",
       " ('312977', 0.9857215881347656),\n",
       " ('170725', 0.9857215881347656),\n",
       " ('401079', 0.9857215881347656),\n",
       " ('150848', 0.98542320728302),\n",
       " ('174269', 0.9849538803100586),\n",
       " ('337709', 0.9849538803100586),\n",
       " ('1673', 0.9849538803100586),\n",
       " ('11392', 0.9849538803100586),\n",
       " ('91603', 0.9849538803100586),\n",
       " ('706308', 0.9849538803100586),\n",
       " ('25197', 0.9849538803100586),\n",
       " ('512007', 0.9849538803100586),\n",
       " ('688186', 0.9849538803100586),\n",
       " ('501341', 0.9849538803100586),\n",
       " ('225713', 0.9849538803100586),\n",
       " ('37922', 0.9849538803100586),\n",
       " ('135514', 0.9849538803100586),\n",
       " ('280208', 0.9849538803100586),\n",
       " ('279235', 0.9849538803100586),\n",
       " ('150173', 0.9849538803100586),\n",
       " ('239631', 0.9839617013931274),\n",
       " ('132400', 0.9832000136375427),\n",
       " ('441159', 0.9832000136375427),\n",
       " ('154632', 0.9832000136375427),\n",
       " ('443225', 0.9832000136375427),\n",
       " ('173903', 0.9832000136375427),\n",
       " ('124383', 0.9832000136375427),\n",
       " ('601803', 0.9832000136375427),\n",
       " ('384531', 0.9832000136375427),\n",
       " ('685399', 0.9832000136375427),\n",
       " ('68090', 0.9832000136375427),\n",
       " ('524049', 0.9832000136375427),\n",
       " ('132943', 0.9832000136375427),\n",
       " ('634920', 0.9832000136375427),\n",
       " ('180244', 0.9832000136375427),\n",
       " ('273522', 0.9832000136375427),\n",
       " ('9994', 0.9832000136375427),\n",
       " ('145850', 0.9832000136375427),\n",
       " ('52293', 0.9832000136375427),\n",
       " ('701901', 0.9832000136375427),\n",
       " ('280241', 0.9832000136375427),\n",
       " ('611791', 0.9832000136375427),\n",
       " ('340731', 0.9832000136375427),\n",
       " ('673941', 0.9832000136375427)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_id = 300\n",
    "random_song_name = song_meta.loc[random_id,'song_name']\n",
    "similar_songs = [int(x[0]) for x in s2v_model.most_similar(str(random_id), topn = 50)]\n",
    "print(random_song_name)\n",
    "song_meta.loc[similar_songs, :]\n",
    "s2v_model.most_similar(str(random_id), topn = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23015it [02:43, 141.01it/s]\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "for index, q in tqdm(val.iterrows()):\n",
    "    try:\n",
    "        most_id = [x[0] for x in p2v_model.most_similar(str(q['id']), topn=200)]\n",
    "        get_song = []\n",
    "        get_tag = []\n",
    "        for ID in most_id:\n",
    "            get_song += song_dic[int(ID)]\n",
    "            get_tag += tag_dic[int(ID)]\n",
    "        get_song = list(pd.value_counts(get_song)[:200].index)\n",
    "        get_tag = list(pd.value_counts(get_tag)[:20].index)\n",
    "        answers.append({\n",
    "            \"id\": q[\"id\"],\n",
    "            \"songs\": remove_seen(q[\"songs\"], get_song)[:100],\n",
    "            \"tags\": remove_seen(q[\"tags\"], get_tag)[:10],\n",
    "        })\n",
    "    except:\n",
    "        answers.append({\n",
    "          \"id\": most_results.loc[index][\"id\"],\n",
    "          \"songs\": most_results.loc[index]['songs'],\n",
    "          \"tags\": most_results.loc[index][\"tags\"],\n",
    "        }) \n",
    "\n",
    "# check and update answer\n",
    "for n, q in enumerate(answers):\n",
    "    if len(q['songs'])!=100:\n",
    "        answers[n]['songs'] += remove_seen(q['songs'], most_results.loc[n]['songs'])[:100-len(q['songs'])]\n",
    "    if len(q['tags'])!=10:\n",
    "        answers[n]['tags'] += remove_seen(q['tags'], most_results.loc[n]['tags'])[:10-len(q['tags'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(answers, \"hoho_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class로 만든 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 목표 : 플레이리스트(노래, 태그, 플레이리스트 제목)이 주어지면, K개의 feature를 뽑도록 한다.\n",
    "# 자연어 처리 모델을 구축\n",
    "class PlaylistEmbedding:\n",
    "    def __init__(self):\n",
    "        # 파일에서 긁어온다.\n",
    "        # 긁어올 파일 : train, val, song_meta\n",
    "        self.train = pd.read_json('./data/train.json', encoding='UTF-8')\n",
    "        self.val = pd.read_json('./data/val.json', encoding='UTF-8')\n",
    "        self.song_meta = pd.read_json('./data/song_meta.json', encoding='UTF-8')\n",
    "        self.most_results = pd.read_json('results.json', encoding='UTF-8')\n",
    "        self.min_count = 10\n",
    "        self.size = 100\n",
    "        self.windows = 100\n",
    "        self.sg = 1\n",
    "        self.p2v_model = WordEmbeddingsKeyedVectors(self.size)\n",
    "    \n",
    "    #플레이리스트 : (플레이리스트 제목과 모든 송의 제목을 gensim으로 preprocess, 장르도 우겨넣자) \n",
    "    def get_dic(self, train, val, song_meta):\n",
    "        song_dic = {} # plylst id - song id\n",
    "        song_name_dic = {} # plylst id - [preprocessed song name]\n",
    "        tag_dic = {} # plylst id - tag\n",
    "        data = pd.concat([train, val])\n",
    "        data = data.set_index('id')\n",
    "        \n",
    "        song_dic = data['songs'].to_dict()\n",
    "        tag_dic = data['tags'].to_dict()\n",
    "        data['song_name_token'] = data['songs'].map(lambda x : sum(list(map(lambda xx : preprocess_string(song_meta.loc[xx, 'song_name']), x)), []))\n",
    "        song_name_dic = data['song_name_token'].to_dict()\n",
    "        \n",
    "        '''\n",
    "        for index, q in tqdm(data.iterrows()):\n",
    "            song_name_dic[str(q['id'])] = sum(list(map(lambda x : preprocess_string(song_meta.loc[x]['song_name']), q['songs'])),[])\n",
    "            song_dic[str(q['id'])] = q['songs']\n",
    "            tag_dic[str(q['id'])] = q['tags']\n",
    "        '''\n",
    "        self.song_dic = song_dic\n",
    "        self.song_name_dic = song_name_dic\n",
    "        self.tag_dic = tag_dic\n",
    "        \n",
    "        data = data.reset_index()\n",
    "        total = data.apply(lambda x : song_name_dic[x['id']] + tag_dic[x['id']] + preprocess_string(x['plylst_title']), axis = 1)\n",
    "        # total = [x for x in total if len(x)>1]\n",
    "        self.total = total\n",
    "    \n",
    "    # word2vec 모델\n",
    "    def get_w2v(self):\n",
    "        w2v_model = Word2Vec(self.total, min_count = self.min_count, size = self.size, window = self.windows, sg = self.sg)\n",
    "        self.w2v_model = w2v_model\n",
    "    \n",
    "    # word2vec을 적합하고 embedded vector를 return\n",
    "    def update_p2v(self, train, val,w2v_model):\n",
    "        ID = []   \n",
    "        vec = []\n",
    "        embedd = {}\n",
    "        for index, q in tqdm(pd.concat([train, val]).iterrows()):\n",
    "            tmp_vec = 0\n",
    "            for song_word in self.song_name_dic[str(q['id'])]:\n",
    "                try:\n",
    "                    tmp_vec += w2v_model.wv.get_vector(song_word)\n",
    "                except KeyError:\n",
    "                        pass\n",
    "            for tag in self.tag_dic[str(q['id'])]:\n",
    "                try:\n",
    "                    tmp_vec += w2v_model.wv.get_vector(tag)\n",
    "                except KeyError:\n",
    "                        pass\n",
    "            for title_word in preprocess_string(q['plylst_title']):\n",
    "                #print(q['plylst_title'])\n",
    "                try:\n",
    "                    tmp_vec += w2v_model.wv.get_vector(title_word)\n",
    "                except KeyError:\n",
    "                        pass\n",
    "                    \n",
    "            if type(tmp_vec) != int:\n",
    "                embedd[str(q['id'])] = tmp_vec\n",
    "                ID.append(str(q['id']))    \n",
    "                vec.append(tmp_vec)\n",
    "                \n",
    "        self.embedd = embedd\n",
    "        self.p2v_model.add(ID, vec)\n",
    "        return embedd\n",
    "    \n",
    "    def get_results(self):\n",
    "        answers = []\n",
    "        for index, q in tqdm(self.val.iterrows()):\n",
    "            try:\n",
    "                most_id = [x[0] for x in self.p2v_model.most_similar(str(q['id']), topn=200)]\n",
    "                get_song = []\n",
    "                get_tag = []\n",
    "                for ID in most_id:\n",
    "                    get_song += self.song_dic[int(ID)]\n",
    "                    get_tag += self.tag_dic[int(ID)]\n",
    "                get_song = list(pd.value_counts(get_song)[:200].index)\n",
    "                get_tag = list(pd.value_counts(get_tag)[:20].index)\n",
    "                answers.append({\n",
    "                    \"id\": q[\"id\"],\n",
    "                    \"songs\": remove_seen(q[\"songs\"], get_song)[:100],\n",
    "                    \"tags\": remove_seen(q[\"tags\"], get_tag)[:10],\n",
    "                })\n",
    "            except:\n",
    "                answers.append({\n",
    "                  \"id\": self.most_results.loc[index][\"id\"],\n",
    "                  \"songs\": self.most_results.loc[index]['songs'],\n",
    "                  \"tags\": self.most_results.loc[index][\"tags\"],\n",
    "                }) \n",
    "                \n",
    "        # check and update answer\n",
    "        for n, q in enumerate(answers):\n",
    "            if len(q['songs'])!=100:\n",
    "                answers[n]['songs'] += remove_seen(q['songs'], self.most_results.loc[n]['songs'])[:100-len(q['songs'])]\n",
    "            if len(q['tags'])!=10:\n",
    "                answers[n]['tags'] += remove_seen(q['tags'], self.most_results.loc[n]['tags'])[:10-len(q['tags'])]  \n",
    "        self.answers = answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist = PlaylistEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist.get_dic(playlist.train, playlist.val, playlist.song_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist.get_w2v()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist.update_p2v(playlist.train, playlist.val, playlist.w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlist.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(playlist.answers, \"results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
